# Assessment Guide

## Course Philosophy on Assessment

**Traditional tests don't work well for simulation-based learning.**

Instead, assessment focuses on:
- **Understanding** over memorization
- **Explanation** over calculation
- **Exploration** over rote completion
- **Creativity** over conformity

## Assessment Framework

### Three Pillars of Success

#### 1. Conceptual Understanding
Can you explain what's happening and why?

#### 2. Experimental Thinking
Can you design and run meaningful experiments?

#### 3. Creative Application
Can you build new simulations or adapt existing ones?

## Assessment Methods

### 1. Reflection Questions (Per Module)

After each module, students should be able to answer:

**Explain**
- What is this simulation modeling?
- What are the key rules/parameters?
- Why does the system behave this way?

**Predict**
- What will happen if I change X?
- What patterns should emerge?
- What's the relationship between variables?

**Connect**
- How does this relate to the real world?
- What other systems behave similarly?
- What are the limitations of this model?

### 2. Experimental Notebooks

Students maintain a notebook documenting:

**For Each Experiment:**
- Hypothesis (what I predict)
- Parameters used
- Observations (what happened)
- Analysis (why it happened)
- Next question (what to try next)

**Example Entry:**
```
Experiment: Effect of growth rate on population

Hypothesis: Higher growth rate â†’ faster population increase

Parameters:
- growth_rate = 0.05, 0.10, 0.15
- initial_population = 100
- time_steps = 100

Observations:
- 0.05: Reached 432 at step 100
- 0.10: Reached 1,803 at step 100
- 0.15: Reached 7,458 at step 100

Analysis:
Doubling growth rate more than doubles final population!
This is exponential growth - small rate changes have huge impacts.

Next question:
What growth rate leads to exactly 1000 at step 100?
```

### 3. Module Challenges

Each module includes exercises that test understanding:

**Completion Criteria:**
- Code runs without errors
- Results are reasonable
- Student can explain the results
- Parameters were experimented with

**Not Required:**
- Perfect code style
- Optimal implementation
- Matching exact numbers

### 4. Final Projects

Choose one:

#### Option A: Deep Dive
Pick one module and go deeper:
- Add complexity to the model
- Test a research question
- Compare to real-world data
- Write up findings

#### Option B: New Simulation
Create a simulation of your own:
- Define the system and rules
- Implement in code
- Visualize results
- Explain insights gained

#### Option C: Multi-System
Combine concepts from multiple modules:
- Integrate different ideas
- Create something novel
- Show emergent behavior

**Project Requirements:**
1. Clear documentation
2. Working code
3. Visualizations
4. Written explanation of insights
5. Reflection on process

### 5. Peer Discussion

Students explain their work to peers:
- Demonstrate their simulation
- Explain their findings
- Answer questions
- Discuss applications

**Assessment:** Can they communicate clearly?

## Rubric for Understanding

### Excellent Understanding
- Explains behavior in own words clearly
- Makes accurate predictions
- Designs meaningful experiments
- Identifies connections to real systems
- Asks insightful "what if" questions

### Good Understanding
- Explains basic concepts correctly
- Makes reasonable predictions
- Runs experiments systematically
- Sees some real-world connections
- Modifies parameters meaningfully

### Developing Understanding
- Describes what code does literally
- Struggles to predict outcomes
- Experiments randomly
- Connections superficial
- Needs guidance to explore

### Needs Support
- Can't explain simulation behavior
- No predictions or all wrong
- Doesn't experiment
- No connections made
- Stuck on technical issues

## Grading Guidelines

### If Grades Are Required

**Participation (30%)**
- Completes module exercises
- Maintains experimental notebook
- Engages with material

**Understanding (40%)**
- Reflection question responses
- Quality of explanations
- Prediction accuracy improves over time

**Project (30%)**
- Working simulation
- Depth of exploration
- Quality of communication
- Creativity and effort

### Better Alternative: Specifications Grading

**Pass/Fail on specific competencies:**

âœ… Can implement a basic simulation
âœ… Can visualize simulation results
âœ… Can explain emergent behavior
âœ… Can design parameter experiments
âœ… Can predict simulation outcomes
âœ… Can connect simulations to real systems
âœ… Completed final project

All âœ… = Full mastery
Most âœ… = Strong understanding
Some âœ… = Developing
Few âœ… = Needs more time

## Self-Assessment Questions

Students regularly reflect:

**Knowledge**
- What concepts do I understand well?
- What am I still confused about?
- What surprised me most?

**Skills**
- What can I code independently?
- What do I need help with?
- How have my debugging skills improved?

**Attitude**
- Am I curious and asking questions?
- Do I experiment beyond requirements?
- Am I thinking like a scientist?

## Portfolio Approach

Students create a portfolio containing:

1. **Code samples** from each module
2. **Experimental notebook** with entries
3. **Visualizations** they created
4. **Final project** with documentation
5. **Reflection essay** on learning journey

Portfolio demonstrates growth over time.

## Red Flags vs Green Flags

### ðŸš© Red Flags
- Only runs code once without exploring
- Can't explain why results occur
- Frustrated when code doesn't work perfectly
- Avoids experimentation
- Wants "the right answer"

### ðŸŸ¢ Green Flags
- Tries multiple parameter values
- Notices and investigates patterns
- Curious about "what if" scenarios
- Comfortable with iteration
- Sees mistakes as learning opportunities

## Intervention Strategies

### If Student Struggles:

**With Concepts:**
- Return to simpler examples
- Use physical analogies
- Draw diagrams together
- Pair with peer mentor

**With Code:**
- Focus on one piece at a time
- Add print statements to see what's happening
- Start with working code, modify gradually
- Don't worry about elegance

**With Motivation:**
- Connect to their interests
- Choose relevant real-world examples
- Celebrate small successes
- Make it playful

## Assessment Timeline

### Weekly
- Complete module exercises
- Add entries to experimental notebook

### Bi-weekly
- Reflection question responses
- Quick check-ins

### End of Course
- Final project
- Portfolio review
- Peer presentations

## Success Criteria

A student has succeeded if they:

1. **Understand** how simulations reveal system behavior
2. **Can build** simple simulations from scratch
3. **Think experimentally** about parameters and outcomes
4. **Communicate** insights in plain language
5. **See connections** between models and reality
6. **Are curious** about complex systems

**Most importantly: They want to keep exploring!**

## Beyond the Grade

The real assessment is:
- Will they use simulation to understand future problems?
- Have they developed computational thinking?
- Do they see the world as systems of interacting rules?
- Are they more comfortable with complexity?

These are the lasting outcomes that matter.

---

## For Instructors

### Assessment Principles

1. **Value exploration** over correctness
2. **Reward curiosity** and experimentation
3. **Assess understanding** through explanation
4. **Encourage iteration** and revision
5. **Make it low-stakes** to reduce anxiety
6. **Focus on growth** not perfection

### Keep It Human

This is about helping young people:
- Think computationally
- Understand complexity
- Build confidence with code
- See systems everywhere

Grades are a tool, not the goal.

### Flexibility

Adapt assessment to your context:
- Formal classroom
- Informal workshop
- Self-paced online
- Summer camp

The key: Did they learn to think with code?
